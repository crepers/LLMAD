{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interactive Machine Data Analysis\n",
        "This notebook demonstrates how to run the LLMAD anomaly detection on Machine data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Load Environment Variables\n",
        "\n",
        "First, we load the necessary environment variables from the `.env` file, such as API keys and the model engine to be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# You can check the loaded variables here\n",
        "print(f\"Using Model: {os.getenv('MODEL_ENGINE')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Set Experiment Parameters\n",
        "\n",
        "Configure the parameters for the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 100\n",
        "PROMPT_MODE = 5 # For Machine dataset\n",
        "DATA_ROOT_DIR = \"data/air\"\n",
        "SAVE_DIR = \"result/machine_test\"\n",
        "VALUE_COL = \"param1\"\n",
        "LABEL_COL = \"label\"\n",
        "TEST_RATIO = 1.0\n",
        "RETRIEVE_POSITIVE_NUM = 0\n",
        "RETRIEVE_NEGATIVE_NUM = 0\n",
        "DATA_DESCRIPTION = \"The data contains sensor readings from a machine.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# Get model name for suffix from environment variables\n",
        "model_engine_name = os.getenv(\"MODEL_ENGINE\", \"unknown_model\")\n",
        "# Extract the main part of the model name for the suffix (e.g., 'gpt' or 'gemini')\n",
        "model_suffix = model_engine_name.split('-')[0]\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
        "\n",
        "# Construct the run name for organization\n",
        "RUN_NAME = f\"Machine_interactive_prompt_{PROMPT_MODE}_win_{WINDOW_SIZE}_{model_suffix}_{timestamp}\"\n",
        "\n",
        "# Create the target directory for results\n",
        "RESULT_DIR = os.path.join(SAVE_DIR, RUN_NAME)\n",
        "os.makedirs(RESULT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Results will be saved in: {RESULT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run Inference\n",
        "Run the LLMAD model on the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python src/main.py \\\n",
        "    --infer_data_path {DATA_ROOT_DIR} \\\n",
        "    --retreive_data_path {DATA_ROOT_DIR} \\\n",
        "    --sub_company all \\\n",
        "    --window_size {WINDOW_SIZE} \\\n",
        "    --prompt_mode {PROMPT_MODE} \\\n",
        "    --result_save_dir {SAVE_DIR} \\\n",
        "    --run_name {RUN_NAME} \\\n",
        "    --value_col {VALUE_COL} \\\n",
        "    --label_col {LABEL_COL} \\\n",
        "    --prompt_extra_cols Machine Stage \\\n",
        "    --data_description \"{DATA_DESCRIPTION}\" \\\n",
        "    --no_affine_transform \\\n",
        "    --retrieve_positive_num {RETRIEVE_POSITIVE_NUM} \\\n",
        "    --retrieve_negative_num {RETRIEVE_NEGATIVE_NUM} \\\n",
        "    --cross_retrieve False \\\n",
        "    --test_ratio {TEST_RATIO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluate Metrics\n",
        "\n",
        "After the detection process is complete, run the evaluation script to calculate performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python Eval/Eval_machine.py --path {RESULT_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Visualize the Results Directly\n",
        "\n",
        "This section provides the code to visualize the results directly within this notebook. The code below will find the `predict.csv` files in the result directory and plot them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "%matplotlib inline\n",
        "\n",
        "# Find all predict.csv files in the result directory\n",
        "predict_files = glob.glob(os.path.join(RESULT_DIR, '**', 'predict.csv'), recursive=True)\n",
        "\n",
        "print(f\"Searching for result files in: {RESULT_DIR}\")\n",
        "print(f\"Found {len(predict_files)} result file(s) to visualize.\")\n",
        "\n",
        "for file_path in predict_files:\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        if df.empty:\n",
        "            print(f\"Skipping empty file: {file_path}\")\n",
        "            continue\n",
        "            \n",
        "        # Extract file name/directory from path\n",
        "        file_name = os.path.basename(os.path.dirname(file_path))\n",
        "        \n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.plot(df['value'], label='Value', alpha=0.7, color='blue')\n",
        "        \n",
        "        # Highlight anomalies\n",
        "        if 'predict' in df.columns:\n",
        "            anomalies = df[df['predict'] == 1]\n",
        "            plt.scatter(anomalies.index, anomalies['value'], color='red', label='Predicted Anomaly', zorder=5)\n",
        "            \n",
        "        if 'label' in df.columns:\n",
        "            true_anomalies = df[df['label'] == 1]\n",
        "            plt.scatter(true_anomalies.index, true_anomalies['value'], marker='x', color='green', label='True Anomaly', s=50, zorder=6)\n",
        "        \n",
        "        # Add metadata to title if available\n",
        "        title = f'Anomaly Detection Results: {file_name}'\n",
        "        if 'Machine' in df.columns and not df['Machine'].empty:\n",
        "            machine = df['Machine'].iloc[0]\n",
        "            title += f\" | Machine: {machine}\"\n",
        "        if 'Stage' in df.columns and not df['Stage'].empty:\n",
        "            stage = df['Stage'].iloc[0]\n",
        "            title += f\" | Stage: {stage}\"\n",
        "            \n",
        "        plt.title(title)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"First 5 rows of {file_name}:\")\n",
        "        display(df.head())\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error visualizing {file_path}: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}